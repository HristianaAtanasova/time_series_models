{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c41fc9-2a56-4086-a124-9dd4010ada30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial selected features:\n",
      " ['rating_checkin', 'latitude', 'bedrooms', 'bathrooms', 'max_guests', 'adr_avg']\n",
      "\n",
      "Final selected features:\n",
      " ['rating_checkin', 'latitude', 'bedrooms', 'bathrooms', 'max_guests', 'adr_avg', 'year', 'month', 'day_of_week', 'property_type', 'superhost', 'available']\n",
      "Best Hyperparameters: n_estimators=300, max_depth=40, min_samples_split=20\n",
      "Best R² Score: 0.78\n",
      "\n",
      "Model Evaluation Results:\n",
      "Mean Absolute Error (MAE): 22.24\n",
      "Mean Squared Error (MSE): 2299.15\n",
      "Root Mean Squared Error (RMSE): 47.95\n",
      "R² Score: 0.78\n",
      "\n",
      "Feature Importance:\n",
      "                                      Feature  Importance\n",
      "5                                     adr_avg    0.296049\n",
      "7                                       month    0.294980\n",
      "16                            superhost_False    0.124594\n",
      "6                                        year    0.120922\n",
      "1                                    latitude    0.042563\n",
      "8                                 day_of_week    0.038284\n",
      "4                                  max_guests    0.021394\n",
      "18                            available_False    0.011660\n",
      "0                              rating_checkin    0.010575\n",
      "3                                   bathrooms    0.009463\n",
      "15    property_type_Private room in townhouse    0.007683\n",
      "19                             available_True    0.005351\n",
      "2                                    bedrooms    0.004823\n",
      "10                  property_type_Entire home    0.004812\n",
      "9                  property_type_Entire condo    0.003857\n",
      "17                             superhost_True    0.001946\n",
      "12           property_type_Entire rental unit    0.000466\n",
      "13  property_type_Private room in guest suite    0.000452\n",
      "14         property_type_Private room in home    0.000111\n",
      "11                  property_type_Entire loft    0.000017\n",
      "\n",
      "Future Price Predictions:\n",
      "     property_id       date  adr_avg  predicted_price\n",
      "8365    10012888 2025-03-03    180.0       180.593784\n",
      "8366    10012888 2025-03-04    180.0       180.593784\n",
      "8367    10012888 2025-03-05    180.0       180.593784\n",
      "8368    10012888 2025-03-06    180.0       180.593784\n",
      "8369    10012888 2025-03-07    180.0       180.593784\n",
      "8370    10012888 2025-03-08    180.0       180.593784\n",
      "8371    10012888 2025-03-09    180.0       180.593784\n",
      "8372    10012888 2025-03-10    180.0       180.593784\n",
      "8373    10012888 2025-03-11    180.0       180.593784\n",
      "8374    10012888 2025-03-12    180.0       180.593784\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the datasets\n",
    "properties_path = \"properties.csv\"\n",
    "prices_path = \"prices.csv\"\n",
    "\n",
    "properties_df = pd.read_csv(properties_path)\n",
    "prices_df = pd.read_csv(prices_path)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 1: Data Cleaning & Merging\n",
    "# --------------------------------------------------------------------\n",
    "# Convert 'price' and 'adr_avg' to numeric, handling errors\n",
    "properties_df['adr_avg'] = pd.to_numeric(properties_df['adr_avg'], errors='coerce')\n",
    "prices_df['price'] = pd.to_numeric(prices_df['price'], errors='coerce')\n",
    "\n",
    "# Convert 'number_of_reviews' to numeric\n",
    "properties_df['number_of_reviews'] = pd.to_numeric(properties_df['number_of_reviews'], errors='coerce')\n",
    "\n",
    "review_columns = [\n",
    "    'review_score', 'rating_accuracy', 'rating_cleanliness', 'rating_checkin',\n",
    "    'rating_location', 'rating_value', 'rating_communication'\n",
    "]\n",
    "\n",
    "# Impute missing ratings with the median of each review column\n",
    "for col in review_columns:\n",
    "    if col in properties_df.columns:\n",
    "        # Compute the median rating for the column\n",
    "        median_rating = properties_df[col].median()\n",
    "        \n",
    "        # Replace 0.0 ratings with the median for entries with no reviews (number_of_reviews == 0)\n",
    "        properties_df.loc[(properties_df[col] == 0.0) & (properties_df['number_of_reviews'] == 0), col] = median_rating\n",
    "\n",
    "# Standardize 'property_id' format by removing 'airbnb_' prefix in prices dataset\n",
    "prices_df['property_id'] = prices_df['property_id'].str.replace(r'airbnb_', '', regex=True)\n",
    "\n",
    "# Convert 'property_id' in both datasets to string to prevent mismatches\n",
    "properties_df[\"property_id\"] = properties_df[\"property_id\"].apply(lambda x: f\"{int(float(x))}\" if pd.notna(x) else x)\n",
    "### prices_df[\"property_id\"] = prices_df[\"property_id\"].astype(str) # this line is redundant \n",
    "\n",
    "# Merge both datasets on 'property_id'\n",
    "merged_df = pd.merge(prices_df, properties_df, on='property_id', how='left', indicator=True)\n",
    "\n",
    "# Convert 'date' to datetime format\n",
    "merged_df[\"date\"] = pd.to_datetime(merged_df[\"date\"])\n",
    "\n",
    "# Handle missing values: Drop rows where 'price' or 'adr_avg' is NaN\n",
    "merged_df.dropna(subset=['price', 'adr_avg'], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 3: Feature Engineering \n",
    "# --------------------------------------------------------------------\n",
    "# Extract time-based features\n",
    "merged_df[\"year\"] = merged_df[\"date\"].dt.year\n",
    "merged_df[\"month\"] = merged_df[\"date\"].dt.month\n",
    "merged_df[\"day_of_week\"] = merged_df[\"date\"].dt.dayofweek\n",
    "\n",
    "# Sine and cosine transformations for cyclical features\n",
    "merged_df['month_sin'] = np.sin(2 * np.pi * merged_df['month'] / 12)\n",
    "merged_df['month_cos'] = np.cos(2 * np.pi * merged_df['month'] / 12)\n",
    "\n",
    "merged_df['day_of_week_sin'] = np.sin(2 * np.pi * merged_df['day_of_week'] / 7)\n",
    "merged_df['day_of_week_cos'] = np.cos(2 * np.pi * merged_df['day_of_week'] / 7)\n",
    "\n",
    "# Define the categorical columns \n",
    "# categorical_columns = ['property_type', 'superhost', 'city'] ### city is redundant because there is only one city...\n",
    "categorical_columns = ['property_type', 'superhost', 'available'] ### city is redundant because there is only one city...\n",
    "\n",
    "# Handle missing categorical values by replacing them with 'unknown' category\n",
    "for col in categorical_columns:\n",
    "    merged_df[col] = merged_df[col].astype(str).fillna('unknown')\n",
    "\n",
    "# Compute price deviation as (price - base price)\n",
    "merged_df[\"price_deviation\"] = merged_df[\"price\"] - merged_df[\"adr_avg\"]\n",
    "\n",
    "# Define cutoff date: Use the current date as the cutoff date\n",
    "# This ensures that training data includes only past data and future data is used for predictions.\n",
    "cutoff_date = datetime.now()  # Current date as the cutoff point\n",
    "\n",
    "### I am not sure if this splitting is correct. Where does the future data come from? \n",
    "# Split into train (past dates only) and future (for prediction)\n",
    "train_df = merged_df[merged_df[\"date\"] < cutoff_date]\n",
    "future_df = merged_df[merged_df[\"date\"] >= cutoff_date]  # Reserved for final predictions\n",
    "\n",
    "# Feature Selection\n",
    "numeric_df = merged_df.select_dtypes(include=['number'])\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Set correlation threshold\n",
    "corr_threshold = 0.2\n",
    "\n",
    "# Select only highly correlated features with price\n",
    "selected_features = corr_matrix.loc['price', (corr_matrix.loc['price']) > corr_threshold].index.tolist()\n",
    "\n",
    "# Remove 'price' from feature list if it appears\n",
    "if 'price' in selected_features:\n",
    "    selected_features.remove('price')\n",
    "\n",
    "print(\"Initial selected features:\\n\", selected_features)\n",
    "\n",
    "# Set up the feature matrix and target\n",
    "features = selected_features + ['year', 'month', 'day_of_week'] + categorical_columns\n",
    "target = \"price_deviation\"\n",
    "\n",
    "print(\"\\nFinal selected features:\\n\", features)\n",
    "\n",
    "# Drop rows with missing values in selected features for training\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 4: Update Feature Selection Pipeline with Encoding and Scaling\n",
    "# --------------------------------------------------------------------\n",
    "# Define the preprocessor: \n",
    "# - OneHotEncoder for categorical variables\n",
    "# - StandardScaler for numerical variables (scaling)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), selected_features + ['year', 'month', 'day_of_week']),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ])\n",
    "### It was probably wrong to include 'year' into the standard scaler because it distorts its meaning..\n",
    "\n",
    "# Create a pipeline with preprocessing and model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1))\n",
    "])\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Step 5: Train-test split\n",
    "#--------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[features], train_df[target], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 6: Hyperparameter Tuning\n",
    "# --------------------------------------------------------------------\n",
    "# Define a simple range for hyperparameters\n",
    "# Chose a basic grid search due to time and computational limitations.\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30, 40],  # Maximum depth of the tree\n",
    "    'min_samples_split': [5, 10, 15, 20]  # Minimum samples required to split a node\n",
    "}\n",
    "\n",
    "# Define the categorical columns that need encoding\n",
    "categorical_columns = ['property_type', 'superhost', 'available']\n",
    "\n",
    "# Manually perform hyperparameter tuning (simple grid search)\n",
    "best_model = None\n",
    "best_score = -np.inf  # Initialize best score\n",
    "\n",
    "# Loop over the parameter grid and evaluate each combination\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            # Create the model with the current hyperparameters\n",
    "            model = RandomForestRegressor(n_estimators=n_estimators, \n",
    "                                          max_depth=max_depth, \n",
    "                                          min_samples_split=min_samples_split, \n",
    "                                          random_state=42, n_jobs=1)\n",
    "            \n",
    "            # Create the full pipeline\n",
    "            model_pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('regressor', model)\n",
    "            ])\n",
    "            \n",
    "            # Fit the model\n",
    "            model_pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate the model on the test set\n",
    "            y_pred = model_pipeline.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # If this combination gives a better R² score, save the model\n",
    "            if r2 > best_score:\n",
    "                best_score = r2\n",
    "                best_model = model_pipeline\n",
    "\n",
    "# Display the best hyperparameters and R² score\n",
    "print(f\"Best Hyperparameters: n_estimators={n_estimators}, max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
    "print(f\"Best R² Score: {best_score:.2f}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 7: Model Evaluation\n",
    "# --------------------------------------------------------------------\n",
    "# Use the best model for prediction\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display evaluation results\n",
    "evaluation_results = {\n",
    "    \"Mean Absolute Error (MAE)\": mae,\n",
    "    \"Mean Squared Error (MSE)\": mse,\n",
    "    \"Root Mean Squared Error (RMSE)\": rmse,\n",
    "    \"R² Score\": r2\n",
    "}\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 8: Feature Importance \n",
    "# -------------------------------------------------------------------- \n",
    "# Get the feature importances from the trained RandomForestRegressor\n",
    "model = model_pipeline.named_steps['regressor']\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing (including categorical features after encoding)\n",
    "encoded_columns = preprocessor.transformers_[1][1].get_feature_names_out(categorical_columns)\n",
    "all_feature_names = selected_features + ['year', 'month', 'day_of_week'] + list(encoded_columns)\n",
    "\n",
    "# Create a DataFrame to display the feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances in the console\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Final Step\n",
    "# --------------------------------------------------------------------\n",
    "# Step 9: Future Price Predictions \n",
    "# --------------------------------------------------------------------\n",
    "future_df = future_df.copy()\n",
    "\n",
    "# Make predictions on future data\n",
    "future_df[\"predicted_price_deviation\"] = best_model.predict(future_df[features])\n",
    "future_df[\"predicted_price\"] = future_df[\"adr_avg\"] + future_df[\"predicted_price_deviation\"]\n",
    "\n",
    "# Display sample future predictions\n",
    "print(\"\\nFuture Price Predictions:\")\n",
    "print(future_df[[\"property_id\", \"date\", \"adr_avg\", \"predicted_price\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f80760-0820-486f-b6b7-0e5ee46fc417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
